---
title: "Retail Shoppers Intention Data Analysis"
author: "Joan Mwangi"
date: "7/3/2021"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, cache = F}
knitr::opts_chunk$set(error = TRUE)
```

# Problem Definition

*Kira Plastinina is a fashion retail store located in China,Russia,
Belarus, Ukraine, Philippines, Kazakhstan, Armenia and Ukraine mostly
targeting ladies between 15 - 25 years.*

*My goal as a data analyst is to get meaningful insights of customers
behavioral patterns and behavior using data collected within the past
one year.*

# Data Sourcing

-   The dataset consists of 10 numerical and 8 categorical attributes.

-   The 'Revenue' attribute will be used as the class label.

    > The following is a breakdown of the data set's variables:
    >
    > -   "Administrative", "Administrative Duration", "Informational",
    >     "Informational Duration", "Product Related" and "Product
    >     Related Duration" represents the number of different types of
    >     pages visited by the visitor in that session and total time
    >     spent in each of these page categories. The values of these
    >     features are derived from the URL information of the pages
    >     visited by the user and updated in real-time when a user takes
    >     an action, e.g. moving from one page to another.
    >
    > -   The "Bounce Rate", "Exit Rate" and "Page Value" features
    >     represent the metrics measured by "Google Analytics" for each
    >     page in the e-commerce site.
    >
    > -   The value of the "Bounce Rate" feature for a web page refers
    >     to the percentage of visitors who enter the site from that
    >     page and then leave ("bounce") without triggering any other
    >     requests to the analytics server during that session.
    >
    > -   The value of the "Exit Rate" feature for a specific web page
    >     is calculated as for all page views to the page, the
    >     percentage that was the last in the session.
    >
    > -   The "Page Value" feature represents the average value for a
    >     web page that a user visited before completing an e-commerce
    >     transaction.
    >
    > -   The "Special Day" feature indicates the closeness of the site
    >     visiting time to a specific special day (e.g. Mother's Day,
    >     Valentine's Day) in which the sessions are more likely to be
    >     finalized with the transaction. The value of this attribute is
    >     determined by considering the dynamics of e-commerce such as
    >     the duration between the order date and delivery date. For
    >     example, for Valentine's day, this value takes a nonzero value
    >     between February 2 and February 12, zero before and after this
    >     date unless it is close to another special day, and its
    >     maximum value of 1 on February 8.
    >
    > -   The dataset also includes the operating system, browser,
    >     region, traffic type, visitor type as returning or new
    >     visitor, a Boolean value indicating whether the date of the
    >     visit is weekend, and month of the year.

# Check the Data

### Loading the dataset

```{r}
online <- read.csv("C:/Users/jojos/Downloads/online_shoppers_intention.csv")
head(online)#previewing the fist 6 rows
```

### Checking the dimensions of the dataset

```{r}
dim(online)
```

\*There are 12330 observations and 18 dimensions

### Checking the datatype

```{r}
str(online)
```

*The dataset has 2 logical columns, 2 character columns, 7 numerical
columns and 6 integer columns*

### Viewing the class of the dataset

```{r}
class(online)
```

*The dataset belongs to the data frame class*

# Perform Data Cleaning

### Checking for duplicates

```{r}
duplicates <- online[duplicated(online),]
dim(duplicates)
```

-   There are 119 duplicates

    #### Removing duplicates

```{r}
new_online <- online[-which(duplicated(online)),]
dim(new_online)
```

### Checking for null values

```{r}
colSums(is.na(new_online))
```

-   Filling null values with mode

```{r}
#first loading the library
library(tidyr)
```

```{r}
gettingmode <- function(x){uniqx <- unique(x)
uniqx[which.max(tabulate(match(x,uniqx)))]}
```

```{r}
#using the mode function to fill in the missing values
new_online[is.na(new_online)] <- gettingmode(new_online[!is.na(new_online)])
#confirming if they have been filled
colSums(is.na(new_online))
```

# Perform Exploratory Data Analysis

Viewing the data type of each column in order to assess which type plots
to visualize

```{r}
sapply(new_online,class)
```

## Univariate Analysis

### Measures of central tendency and dispersion

```{r}
summary(new_online)
```

### Histograms

```{r}
#histogram of page values
 hist(new_online$PageValues,
     main= "Histogram of Page Values",
     xlab="PAGE VALUES",col="darkmagenta",freq=FALSE)

```

*Majority of the average page values visited by the user are around 0*

```{r}
#histogram of administrative duration
 h <- hist(new_online$SpecialDay,
     main= "Histogram of Special Day",
     xlab="Special day ",col="darkmagenta",freq=FALSE)

```

*According to the histogram, majority of the users didn't visit the site
during special days*

```{r}
#histogram of administrative
 hist(new_online$OperatingSystems,
     main= "Histogram of Operating systems",
     xlab="Operating systems",col="darkmagenta",freq=FALSE)

```

*Majority of the operating systems were within a value of 2*

```{r}
#histogram of administratuve
 hist(new_online$Browser,
     main= "Histogram of Browsers",
     xlab="Browsers",col="darkmagenta",freq=FALSE)

```

*Browsers mainly had a value below 2*

```{r}
#histogram of administratuve
 hist(new_online$TrafficType,
     main= "Histogram of Traffic type",
     xlab="Traffic type",breaks=4,col="darkmagenta",xlim=c(0,25),freq=FALSE)
```

*Majority of the values within the traffic type is between 0 and 5*

### Bar plots

```{r,echo=FALSE}
#Change main title and axis titles
#install.packages("DataExplorer")
library(DataExplorer)
```

```{r}
plot_bar(new_online)
```

From the bar plots we can see that;

    -   both administrative and informational columns, they had the highest frequency at 0.

    -   The Month of May had the highest records followed by November.

    -   Returning visitor had the highest record of over 10,000

    -   Weekend type False had the highest record same case as Revenue variable

## Bi-variate Analysis

### Correlation Matrix

```{r}
        #install.packages("corrplot")
        library(corrplot)
```

```{r}
        correlation <- round(cor(new_online[,c(9,10,12,13,14,15)]),2)
        correlation
```

```{r}
        corrplot(correlation,type="upper",order="hclust",tl.col="black",tl.srt=40)
```

The correlation matrix show that:

-   Browser and operating systems are positively correlated with a value
    of 0.21

-   Traffic type and operating system with 0.18

-   Traffic type and browser with 0.1

-   Region and browser with 0.09

-   Region and operating system with 0.07

# Implementing the Solution

# Feature engineering

### Data Pre processing

```{r}
#install.packages("caret")
install.packages("lattice")
install.packages("ggplot2")
library(caret)
```

```{r}
#transforming the logical variables 

new_online$Revenue <- as.numeric(new_online$Revenue)
new_online$Weekend <- as.numeric(new_online$Weekend)
head(new_online)
```

### Label Encoding

```{r}
library(CatEncoders)

  encode <- LabelEncoder.fit(new_online$VisitorType)
  new_online$VisitorType <- transform(encode, new_online$VisitorType)

 encode <- LabelEncoder.fit(new_online$Month)
  new_online$Month <- transform(encode, new_online$Month)
print(unique(new_online$Month))
print(unique(new_online$VisitorType))
```

### scaling the dataset

```{r,echo=FALSE}

#scaling the data, to a mean of 0 and standard deviation of 1
online1 <- new_online[sapply(new_online, is.numeric)]
#head(online1)
online.norm <- as.data.frame(scale(online1))
head(online.norm)
```

## Computing the kmeans clustering in r

```{r}
set.seed(123)
#setting revenue as online.class

online.class <- new_online[,18]
online_kmeans <- kmeans(online.norm,centers=3,nstart=25)

print(online_kmeans)
```

```{r}

online_kmeans$size
```

The first cluster has 7910 values, the second cluster has 1947 values
and the third cluster has 2354 values

## Visualizing the kmeans clusters

```{r}

#installing the packages
install.packages("factoextra")
install.packages("NbClust")
```

```{r}
library(factoextra)
library(NbClust)
library(ggplot2)
fviz_cluster(online_kmeans,data= online.norm)
```

### Determining the optimal number of clusters 

### Method 1 : Elbow Method

```{r}
library(factoextra)
library(NbClust)
library(ggplot2)
```

```{r message=FALSE, warning=FALSE}
fviz_nbclust(x = online.norm,FUNcluster = kmeans, method = 'wss')
```

the number of clusters using the elbow method is 3

### 2. Silhouette Method

```{r}
fviz_nbclust(x = online.norm,FUNcluster = kmeans, method = 'silhouette' )
```

Silhouette method suggests that the optimal number of clusters is 2

```{r}
#given that the the biggest joint is at 8, ill set the clusters at 8 and visualize them
online_k2 <- kmeans(online.norm,2)
fviz_cluster(online_k2,online.norm)
```

using 2 clusters suggested by silhouette method didn't create clear
clusters

## Hierarchical clustering 

### Identifying hierarchical clustering methods that can identify stronger clustering structures

```{r}

# Compute distances using euclidean
d <- dist(scale(online.norm), method = "euclidean")
#hierarchical clustering using ward.d2
hc <- hclust(d, method = "ward.D2")
```

### 

```{r}
#plotting a dendogram 
plot(hc, xlim = c(1, 20), ylim = c(1,8))

```

```{r}
cut <- cutree(clusters, 6)
table(cut,online.class)
#comparing the results gotten with scaled_online.class
```

## finding the optimal number of clusters

```{r}
library(NbClust)
library(tidyverse)  
library(cluster)    
library(factoextra) 
library(dendextend)
fviz_nbclust(online.norm,FUN = hcut, method="wss")
```

```{r}
#using silhoutte method to compare the elbow method and silhoutte
fviz_nbclust(online.norm, FUN = hcut, method = "silhouette")
```

```{r}
plot(hc, cex = 0.6)
rect.hclust(hc, k = 3, border = 2:5)
```

# Challenging the Solution

challenge the solution using in hierarchical clustering using method set
at average

```{r}
clusters1 <- hclust(dist(online.norm),method="average")
plot(clusters)
```

```{r}
cut <- cutree(clusters1, 10)
table(cut,online.class)
```

# Follow up Questions

-   Kmeans clustering is preferred for this type of dataset due its
    dimensions, kmeans also produced higher clusters that are slightly
    distinguishable from the visualization compared to Hierarchical
    clustering

-   However, kmeans requires that the clusters to be predetermined, and
    it was difficult to determine the number of clusters for
    optimization. It was a tie between 2 and 3. Kmeans is also sensitive
    to scaling data, data needs to be scaled first so that all variables
    are equally used during clustering.
