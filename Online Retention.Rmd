---
title: "Retail Shoppers Intention Data Analysis"
author: "Joan Mwangi"
date: "7/3/2021"
output: html_document
---

```{r setup, cache = F}
knitr::opts_chunk$set(error = TRUE)
```

# Problem Definition

*Kira Plastinina is a fashion retail store located in China,Russia, Belarus, Ukraine, Philippines, Kazakhstan, Armenia and Ukraine mostly targeting ladies between 15 - 25 years.*

*My goal as a data analyst is to get meaningful insights of customers behavioral patterns and behavior using data collected within the past one year.*

# Data Sourcing

-   The dataset consists of 10 numerical and 8 categorical attributes.

-   The 'Revenue' attribute will be used as the class label.

    > The following is a breakdown of the data set's variables:
    >
    > -   "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another.
    >
    > -   The "Bounce Rate", "Exit Rate" and "Page Value" features represent the metrics measured by "Google Analytics" for each page in the e-commerce site.
    >
    > -   The value of the "Bounce Rate" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.
    >
    > -   The value of the "Exit Rate" feature for a specific web page is calculated as for all page views to the page, the percentage that was the last in the session.
    >
    > -   The "Page Value" feature represents the average value for a web page that a user visited before completing an e-commerce transaction.
    >
    > -   The "Special Day" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother's Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentine's day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.
    >
    > -   The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.

# Check the Data

### Loading the dataset

```{r}
online <- read.csv("C:/Users/jojos/Downloads/online_shoppers_intention.csv")
head(online)#previewing the fist 6 rows
```

### Checking the dimensions of the dataset

```{r}
dim(online)
```

\*There are 12330 observations and 18 dimensions

### Checking the datatype

```{r}
str(online)
```

*The dataset has 2 logical columns, 2 character columns, 7 numerical columns and 6 integer columns*

### Viewing the class of the dataset

```{r}
class(online)
```

*The dataset belongs to the data frame class*

# Perform Data Cleaning

### Checking for duplicates

```{r}
duplicates <- online[duplicated(online),]
dim(duplicates)
```

-   There are 119 duplicates Removing the duplicates

```{r}
new_online <- online[-which(duplicated(online)),]
dim(new_online)
```

### Checking for null values

```{r}
colSums(is.na(new_online))
```

-   I'll fill them with mode

```{r}
#first loading the library
library(tidyr)
```

```{r}
#creating the mode function
gettingmode <- function(v){
  uniqv <- unique(v)
  mode <- uniqv[which.max(tabulate(match(v, uniqv)))]
  mode}

```

```{r}
#using the mode function to fill in the missing values
new_online[is.na(new_online)] <- gettingmode(new_online[!is.na(new_online)])
#confirming if they have been filled
colSums(is.na(new_online))
```

# Perform Exploratory Data Analysis

Viewing the data type of each column in order to assess which type plots to visualize

```{r}
sapply(new_online,class)
```

## Univariate Analysis

### Measures of central tendency and dispersion

```{r}
summary(new_online)
```

### Histograms

```{r}
#histogram of page values
 hist(new_online$PageValues,
     main= "Histogram of Page Values",
     xlab="PAGE VALUES",col="darkmagenta",freq=FALSE)

```

*Majority of the average page values visited by the user are around 0*

```{r}
#histogram of administrative duration
 h <- hist(new_online$SpecialDay,
     main= "Histogram of Special Day",
     xlab="Special day ",col="darkmagenta",freq=FALSE)

```

*According to the histogram, majority of the users didn't visit the site during special days*

```{r}
#histogram of administratuve
 hist(new_online$OperatingSystems,
     main= "Histogram of Operating systems",
     xlab="Operating systems",col="darkmagenta",freq=FALSE)

```

*Majority of the operating systems were within a value of 2*

```{r}
#histogram of administratuve
 hist(new_online$Browser,
     main= "Histogram of Browsers",
     xlab="Browsers",col="darkmagenta",freq=FALSE)

```

*Browsers mainly had a value below 2*

```{r}
#histogram of administratuve
 hist(new_online$TrafficType,
     main= "Histogram of Traffic type",
     xlab="Traffic type",breaks=4,col="darkmagenta",xlim=c(0,25),freq=FALSE)
```

*Majority of the values within the traffic type is between 0 and 5* \#\#\#Bar plots

```{r,echo=FALSE}
#Change main title and axis titles
#install.packages("DataExplorer")
library(DataExplorer)
```

```{r}
plot_bar(new_online)
```

From the bar plots we can see that;

    -   both administrative and informational columns, they had the highest frequency at 0.

    -   The Month of May had the highest records followed by November.

    -   Returning visitor had the highest record of over 10,000

    -   Weekend type False had the highest record same case as Revenue variable

## Bi-variate Analysis

\#\#\#Correlation Matrix

```{r}
        #install.packages("corrplot")
        library(corrplot)
```

```{r}
        correlation <- round(cor(new_online[,c(9,10,12,13,14,15)]),2)
        correlation
```

```{r}
        corrplot(correlation,type="upper",order="hclust",tl.col="black",tl.srt=40)
```

The correlation matrix show that:

-   Browser and operating systems are positively correlated with a value of 0.21

-   Traffic type and operating system with 0.18

-   Traffic type and browser with 0.1

-   Region and browser with 0.09

-   Region and operating system with 0.07

# Implementing the Solution

\#\#Feature engineering \#\#\#Label Encoding

```{r}
#install.packages("caret")
library(caret)
```

```{r}
#transforming the logical variables variables

new_online$Revenue <- as.numeric(new_online$Revenue)
new_online$Weekend <- as.numeric(new_online$Weekend)
head(new_online)
```

```{r}
library(CatEncoders)

  encode <- LabelEncoder.fit(new_online$VisitorType)
  new_online$VisitorType <- transform(encode, new_online$VisitorType)

 encode <- LabelEncoder.fit(new_online$Month)
  new_online$Month <- transform(encode, new_online$Month)
unique(new_online$Month)
```

```{r,echo=FALSE}
new_online <- sapply(new_online,as.numeric)
```

### Scaling the data

```{r}
#scaling the data by subtracting with the mean and dividing by the standard deviation
scaled_online <- scale(new_online)

head(scaled_online, n=3)
```

```{r}
#Splitting the dataset
scaled_online1 <- scaled_online[,1:17]
scaled_online.class <- scaled_online[,"Revenue"]
head(scaled_online1,n=3)
#previewing the number of records in each center
```

```{r}
#using the kmeans with the number of centroids set at 3
#result <- kmeans(scaled_online1,3)
#result$size
```

```{r}
#installing the packages
#install.packages("factoextra")
#install.packages("NbClust")
```

\#\#\#Optimization using the silhoutte method

```{r}
library(factoextra)
library(NbClust)
```

```{r message=FALSE, warning=FALSE}
fviz_nbclust(x = scaled_online,FUNcluster = kmeans, method = 'wss')
```

```{r}
#given that the the biggest joint is at 8, ill set the clusters at 8 and visualize them
model <- kmeans(scaled_online1,8)
options(repr.plot.width=14, repr.plot.width=9)
fviz_cluster(model,scaled_online1)
```

\#\#Hierarchical clustering

```{r}
d <- dist(scaled_online, method = "euclidean")
#using the wards method for hierarchical clustering
res.hc <- hclust(d, method = "ward.D2")
#plotting the hierarchical clustering
plot(res.hc, cex = 0.6, hang = -1)
```

```{r}
clusters <- hclust(dist(scaled_online1[, 5:6]))
plot(clusters)
```

```{r}
cut <- cutree(clusters, 6)
table(cut,scaled_online.class)
#comparing the results gotten with scaled_online.class, and it can be viewed
```

# Challenging the Solution

challenge the solution using in hierarchical clustering using method set at average

```{r}
clusters1 <- hclust(dist(scaled_online1[, 9:15]),method="average")
plot(clusters)
```

```{r}
cut <- cutree(clusters1, 10)
table(cut,scaled_online.class)
```

using the two columns 9 and 15 which are Page Values and Traffic Type, it managed to classify the revenue class better according the table above compared to using Product related duration and bounce rates columns

# Follow up Questions 

-    Kmeans clustering is preferred for this type of dataset due its dimensions, kmeans also produced higher clusters that can be distinguishable from the visualization compared to Hierarchical clustering

-   However, kmeans requires that the clusters should be predetermined, and it was difficult to determine the number of clusters for optimization. It was a tie between 3 and 8. Kmeans is also sensitive to scaling data, data needs to be scaled first so that all variables are equally used during clustering.
